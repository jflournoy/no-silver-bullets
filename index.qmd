---
title: Understanding Technology Team Velocity at Scale
author:
  - name: John Flournoy
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: true
    email: john-flournoy@pluralsight.com
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
# keywords:
abstract: |
  Things
plain-language-summary: |
  Other things
key-points:
  - A point
date: last-modified
bibliography: references.bib
number-sections: true
acm-metadata:
  # comment this out to make submission anonymous
  
  # if uncommented, this produces a teaser figure
  # 
  # anonymous: true
  # comment this out to build a draft version
  final: false
  # comment this out to specify detailed document options
  # acmart-options: sigconf, review  
  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "Conference acronym 'XX"
  conference-name: |
    Make sure to enter the correct
    conference title from your rights confirmation email
  conference-date: June 03--05, 2018
  conference-location: Woodstock, NY
  price: "15.00"
  isbn: 978-1-4503-XXXX-X/18/06
  # if present, replaces the list of authors in the page header.
        \end{CCSXML}
        
        \ccsdesc[500]{Software and its engineering~Software development process management}
        \ccsdesc[500]{General and reference~Empirical studies}
  shortauthors: Trovato et al.
  # Please copy and paste the code instead of the example below.
           <concept>
         </ccs2012>
  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  ccs: |
        \begin{CCSXML}
        <ccs2012>
           <concept>
               <concept_id>10011007.10011074.10011081</concept_id>
               <concept_desc>Software and its engineering~Software development process management</concept_desc>
               <concept_significance>500</concept_significance>
               </concept>
               <concept_id>10002944.10011123.10010912</concept_id>
               <concept_desc>General and reference~Empirical studies</concept_desc>
               <concept_significance>500</concept_significance>
               </concept>
  keywords:
    - productivity
    - cycle time
    - collaboration
    - individual differences
    - within-person variation
  # teaser:
  #   image: sampleteaser
  #   caption: figure caption
  #   description: teaser description   
editor: 
  markdown: 
    wrap: 72
---

```{r setup}
library(ggplot2)
library(marginaleffects)
library(brms)
library(tidybayes)
library(rlang)
library(sjPlot)
library(ggnewscale)

plot_marginal_effect <- function(x, condition, trans.y = NULL, trans.x = NULL, patchwork_args = list(), ...){
  require(patchwork)
  require(rlang)
  s_to_day <- \(x) return(x / (60*60*24))
  # Extract the formula terms
  terms_object <- attr(model.frame(x), "terms")
  # Get the variables attribute (which includes the response variable)
  response_variable <- as.character(attr(terms_object, "variables"))[2]
  max_resp <- quantile(x$data[[response_variable]], p = .95)
  coords <- s_to_day(c(0, max_resp))
  condition_list <- unique(list(condition, rev(condition)))
  p <- lapply(condition_list, \(c) {
    pp <- marginaleffects::plot_predictions(x, condition = c, re_formula = NA, trans = s_to_day, ...) + 
      scale_color_discrete(labels = function(b) round(as.numeric(b), 2), aesthetics = c('fill', 'color')) + 
      coord_cartesian(y = coords) + 
      theme_minimal()
  
    legend_name_auto <- ggplot_build(pp)$plot$labels$fill
    legend_list <- list(y = 'Median Cycle Time (days)')
    if (!is.null(legend_name_auto)) {
      legend_name_auto <- abbreviate(gsub('_', ' ', legend_name_auto), minlength = 8)
      legend_list <- c(legend_list, list(fill = legend_name_auto, color = legend_name_auto))
    }
  
    pp <- pp + do.call(labs, legend_list)
    
    return(pp)
  })
  
  if(!is.null(trans.y)){
    p <- lapply(p, \(pp) pp + scale_y_continuous(trans = trans.y))
  }
  if(!is.null(trans.x)){
    p <- lapply(p, \(pp) pp + scale_x_continuous(trans = trans.x))
  }
  default_patchwork <- list(nrow = 1)
  p <- do.call(patchwork::wrap_plots, c(list(p), modifyList(default_patchwork, patchwork_args)))
  return(p)
}

theme_clean <- theme_minimal() + 
  theme(
    strip.text.x = element_blank(), 
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(), 
    panel.spacing = unit(0, 'lines')
  )
```

## Introduction

Optimizing the productivity of software developers is one of the most important goals of technology organizations. Developer productivity is a prerequisite for almost anything these organizations hope to accomplish. This is true whether software is the an organizations primary product, or whether internal software engineering enables other end goals (such as in the pharmaceutical industry). Despite the importance of developer productivity, relatively little is understood about the factors that affect it.

RQ1. How do common software development process and workplace factors impact productivity? RQ2. How much between- and within-person variation is there in productivity?

## Background

### Defining productivity

There is disagreement about what constitutes productivity. Perceptions of what counts as productivity differ across individuals and roles, with managers tending to focus on long-term outcomes and ICs focusing on activity, for example \[ref story 2022\]. The scientific literature on this topic has used code-based metrics such as cycle time, commits, or lines of code\[cite\]. These have obvious limitations in that the meaning of a particular unit for any of these metrics may be different depending on context \[cite\]. Some researches have sought solutions to this problem by asking individuals to rate their own level of, or satisfaction with, productivity \[storey2021\]. While it is plausible that perceived productivity could indicate productivity per se, such a measure is not free from context effects that would impact more "objective" metrics, and presents its own set of measurement challenges. Indeed, the discussion to date around productivity reveals what is primarily a philosophical problem: we have not nailed down the necessary and sufficient conditions for something to count as productivity. This is not an easy task, and there are likely several kinds of productivity we may legitimately be interested in. This research project cannot, and will not, attempt to resolve these problems.

In this analysis, we will examine cycle time, defined as the time between the opening of a ticket and its closure. This duration is an index of many disparate causes, as any developer can easily attest. Nevertheless, it represents a unit of work that has been deemed important to the completion of some larger piece of work.

## Research design and methodology

All code can be found in the repository for this paper.

### Data Selection and Characteristics

To examine coding time, task scoping, and collaboration as predictors of cycle time over time, we pulled data from the software metrics tool, Flow. Flow is a software platform that leverages git and ticketing data to track individual, team, and organization-level metrics that can be used as indicators of production, productivity, and performance over time. From Flow, we selected data for analysis if users actively contributed code. The data thus consisted of data from 314 organizations of varying sizes and industries, 9,101 teams, and 37,120 developers.

### Defining Study Variables

Using the most complete data for each user, we use the mean to aggregate each variable at the month level and the year level (see below for more details specific to each variable). For each predictor, we then subtracted each person's yearly average from their monthly data to produce a within-person deviation variable. This allows us to disaggregate effects on the outcome due to yearly-level individual differences and within-person, month-to-month fluctuations.

***Cycle Time*** This is the dependent variable in these analyses. After computing the cycle time for each closed ticket in seconds, we found the median cycle time for each month for each user using all tickets _opened_ in that month. For example, a ticket opened on the 9th of April, and closed on the 3rd of May would contribute 2,246,400 seconds to the calculation of the median for April.

***Unclosed Tickets*** We were not able to observe the closing date for every ticket given our data collection cutoff of ***DATE***, and so it is plausible that we underestimate the median cycle time in a way that depends in part on how many ticket closing times we do not observe. For this reason, we also computed the proportion of tickets opened in that month that had not been closed by the end of our data collection. For example, any ticket opened in April but not closed by ***DATE*** would count toward the proportion of unclosed tickets for that month.

***Coding Time*** The amount of coding time was summarized as the average number of days per week that a developer made at least one commit. We divided the number of coding days in a month by the total number of days in that month and multiplied by seven to aid in interpretation.

***Total Merged PRs*** Task scoping is a potentially important driver of cycle time. To measure the extent to which smaller chunks of work lead to a completed ticket, we counted the number of merged pull requests for each user for each month.

***Percent Defect Tickets*** Defect tickets represent unplanned work that may interfere with timely completion of planned work. To account for this possibility, for each user, for each month, we computed the proportion of tickets that were defect tickets.

***Degree centrality*** We measured collaboration by calculating degree centrality. To evaluate degree centrality, a metric derived from network analysis, we employed a framework where developers were treated as nodes within the network, and their interactions in the form of Pull Requests (PRs) were regarded as connections. Within this analysis, we measured the degree centrality of each developer by assessing the number of other developers with whom they had collaborated throughout the entirety of 2022. To ensure a normalized assessment, this value was then divided by the total number of developers constituting the organizational network. The calculations were executed using the Python package Networkx. This particular variable serves as an effective proxy for quantifying the extent of collaboration amongst developers.

***Comments per PR*** Another indicator of collaboration is the frequency of comments within PRs. We undertook a comprehensive examination of all PRs that were successfully merged in the year 2022, meticulously calculating the average word count contained within each PR. This served as a valuable measure to gauge the depth of collaboration exhibited during the development and review process.

| Broad Construct | Variable                          | Variable Description                                                 |
|------------------------|------------------|------------------------------|
| Coding Time     | Coding Days per Week              | Avg number of coding days a week                                     |
| Task Scoping    | Total Merged PRs                  | Total number of merged PRs per developer                             |
|                 | Percent Defect Tickets            | Percent of all tickets that are defect tickets                       |
| Collaboration   | Degree Centrality                 | Score based on the number of reviewers a developer has worked with   |
|                 | Comments per PR                   | Number of comments per pr a developer is the author on               |
| Productivity    | Cycle Time                        | Avg time from ticket start to end                                    |
|                 | Proportion unclosed tickets       | Control variable to account for tickets missing cycle time           |

: Construct and variable descriptions {#tbl-constructs}

### Analytic Approach

To examine coding days per week, merged PRs, defect tickets, degree centrality, and comments per PR as predictors of cycle time over time, we conducted a multilevel conditional growth model. We used a multilevel growth model due to the hierarchical nature of the data, where each time measurement of cycle time is nested within each developer, and each developer is nested within a team. Multilevel growth models take this hierarchical structure into account by modeling within- and between-person level equations (Curran et al., 2010). We coded time as months in 2022 to capture change in cycle time over the course of a full calendar year. To preserve statistical power, we limited the number of time points to four - one point for each quarter of the calendar year. Given that most organizations set quarterly goals at the beginning of each quarter and push to meet those goals at the end of each quarter, we opted to examine data from the middle month of each quarter (February, May, August, and November). This approach allowed us to capture a more stable and realistic trajectory of change over the course of the year.

As recommended by Singer and Willett (2003), we implemented a three-step procedure. First, we ran an unconditional means model. This model tested for sufficient between group variance to run a multilevel model by calculating intraclass correlation coefficients (ICCs). We then conducted a multilevel conditional growth model, adding a fixed slope representing linear change and nesting individual developers within teams, to assess the effects of time on cycle time (model 1). Finally, to test coding days per week, merged PRs, percent of defect tickets, degree centrality, and comments per PR as predictors of cycle time over time, we conducted a multilevel conditional growth model. In this model, we added the yearly averages of each predictor and their interaction effects with time as time-invariant fixed factors at the individual developer-level (model 2). In this model, significant interaction terms indicate significantly different rates of change in cycle time between groups (Greene, Way, & Pahl, 2006; Taylor et al., 2010; Huang, 2023). The models were fit using restricted maximum likelihood using the lme4 package in R (Bates et al., 2015) and inferential tests for fixed effects were conducted via t-tests (Satterthwaite degrees of freedom) computed using the lmerTest package (Kuznetsova, Brockhoff, & Christensen, 2017).

## Results

```{r prepare results}
ct_model <- readRDS('cycle_time_full_intx_nlq.rds')
plot_dir <- 'plots'
if(!dir.exists(plot_dir)) dir.create(plot_dir)

make_marginal_effect_image <- function(x, condition, image_dir, return_plot = FALSE, width = 8, height = 5, units = 'in', trans.y = NULL, trans.x = NULL, patchwork_args = list(), ...){
  image_path <- file.path(image_dir, paste0(paste(condition, collapse = '__'), '.png'))
  r <- image_path
  if(return_plot | !file.exists(image_path)){
    tryCatch({
      p <- plot_marginal_effect(x = x, condition = condition, 
                                trans.y = trans.y, trans.x = trans.x, patchwork_args = patchwork_args, ...)
      gg <- ggsave(image_path, plot = p, width = width, height = height, units = units)
      if(return_plot){
        attr(r, which = 'plot') <- p
      }
    }, error = function(e) message('Could not make ', image_path, ': ', e))
  }
  return(r)
}

plot_image_filenames <- lapply(
  list(
    'month_num_c',
    'within_quarter_month_num_c',
    c('month_num_c', 'q_unclosed_c'),
    c('month_num_c', 'team_size_c'),
    'wi_avg_coding_days_per_week',
    c('month_num_c', 'yr_avg_avg_coding_days_per_week_c'),
    'wi_total_merged_prs',
    c('month_num_c', 'yr_avg_total_merged_prs_c'),
    'wi_defect_tickets_pct_indiv',
    c('month_num_c', 'yr_avg_defect_tickets_pct_indiv_c'),
    'wi_degree_centrality_monthly_100',
    c('month_num_c', 'yr_avg_degree_centrality_monthly_100_c'),
    'wi_comments_per_pr',
    c('month_num_c', 'yr_avg_comments_per_pr_c')
  ),
  \(x) make_marginal_effect_image(ct_model, x, image_dir = plot_dir, return_plot = FALSE, points = .25)
)
```

```{r model summary table}
atab <- sjPlot::tab_model(ct_model, transform = NULL, show.ci = .95, show.ci50 = TRUE,
                          show.p = FALSE, show.r2 = FALSE, show.icc = FALSE, show.re.var = FALSE, show.ngroups = TRUE,
                          show.obs = TRUE, robust = TRUE,
                          rm.terms = na.omit(stringr::str_extract(variables(ct_model), 'sds_.*')))
atab
```

## Discussion

## Related Work

## Conclusions

## References {.unnumbered}

::: {#refs}
:::
