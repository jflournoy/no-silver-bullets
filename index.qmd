---
title: Understanding Technology Team Velocity at Scale
author:
  - name: John C. Flournoy
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: true
    email: jcflournoyphd@pm.me
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
  - name: Carol S. Lee
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: false
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
  - name: Maggie Wu
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: false
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
  - name: Masoud Hosseini
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: false
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
  - name: Catherine M. Hicks
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: false
    email: cat-hicks@pluralsight.com
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
# keywords:
abstract: |
  Things
plain-language-summary: |
  Other things
key-points:
  - A point
date: last-modified
bibliography: references.bib
number-sections: true
acm-metadata:
  # comment this out to make submission anonymous
  
  # if uncommented, this produces a teaser figure
  # 
  # anonymous: true
  # comment this out to build a draft version
  final: false
  # comment this out to specify detailed document options
  # acmart-options: sigconf, review  
  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: acmlicensed
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "Conference acronym 'XX"
  conference-name: |
    Make sure to enter the correct
    conference title from your rights confirmation email
  conference-date: June 03--05, 2018
  conference-location: Woodstock, NY
  price: "15.00"
  isbn: 978-1-4503-XXXX-X/18/06
  # if present, replaces the list of authors in the page header.
  shortauthors: Trovato et al.
  # Please copy and paste the code instead of the example below.
  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  ccs: |
        \begin{CCSXML}
        <ccs2012>
           <concept>
               <concept_id>10002944.10011123.10010912</concept_id>
               <concept_desc>General and reference~Empirical studies</concept_desc>
               <concept_significance>500</concept_significance>
               </concept>
         </ccs2012>
        \end{CCSXML}
        
        \ccsdesc[500]{General and reference~Empirical studies}
  keywords:
    - productivity
    - cycle time
    - collaboration
    - individual differences
    - within-person variation
  # teaser:
  #   image: sampleteaser
  #   caption: figure caption
  #   description: teaser description   
editor: 
  markdown: 
    wrap: 72
---

```{r setup}
# Function to check if we're running in a GitHub Action
is_github_action <- function() {
  Sys.getenv("GITHUB_ACTIONS") == "true"
}

# Set custom library path if running locally
if (!is_github_action()) {
  .libPaths('/home/rstudio/R/x86_64-pc-linux-gnu-library/4.3/')
}

library(ggplot2)
library(marginaleffects)
library(rstan)
library(brms)
library(tidybayes)
library(rlang)
library(sjPlot)
library(ggnewscale)
library(kableExtra)

plot_marginal_effect <- function(x, condition, trans.y = NULL, trans.x = NULL, patchwork_args = list(), ...){
  require(patchwork)
  require(rlang)
  s_to_day <- \(x) return(x / (60*60*24))
  # Extract the formula terms
  terms_object <- attr(model.frame(x), "terms")
  # Get the variables attribute (which includes the response variable)
  response_variable <- as.character(attr(terms_object, "variables"))[2]
  max_resp <- quantile(x$data[[response_variable]], p = .95)
  coords <- s_to_day(c(0, max_resp))
  condition_list <- unique(list(condition, rev(condition)))
  p <- lapply(condition_list, \(c) {
    pp <- marginaleffects::plot_predictions(x, condition = c, re_formula = NA, trans = s_to_day, ...) + 
      scale_color_discrete(labels = function(b) round(as.numeric(b), 2), aesthetics = c('fill', 'color')) + 
      coord_cartesian(y = coords) + 
      theme_minimal()
  
    legend_name_auto <- ggplot_build(pp)$plot$labels$fill
    legend_list <- list(y = 'Median Cycle Time (days)')
    if (!is.null(legend_name_auto)) {
      legend_name_auto <- abbreviate(gsub('_', ' ', legend_name_auto), minlength = 8)
      legend_list <- c(legend_list, list(fill = legend_name_auto, color = legend_name_auto))
    }
  
    pp <- pp + do.call(labs, legend_list)
    
    return(pp)
  })
  
  if(!is.null(trans.y)){
    p <- lapply(p, \(pp) pp + scale_y_continuous(trans = trans.y))
  }
  if(!is.null(trans.x)){
    p <- lapply(p, \(pp) pp + scale_x_continuous(trans = trans.x))
  }
  default_patchwork <- list(nrow = 1)
  p <- do.call(patchwork::wrap_plots, c(list(p), modifyList(default_patchwork, patchwork_args)))
  return(p)
}

brms_to_draws <- function(x, variables){
  x <- x$fit@sim$samples
  x_subsamples <- lapply(x, \(x) x[, variables])
  x_draws <- posterior::as_draws(x_subsamples)
  return(x_draws)
}

theme_clean <- theme_minimal() + 
  theme(
    strip.text.x = element_blank(), 
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(), 
    panel.spacing = unit(0, 'lines')
  )
```

```{r prepare results}
ct_model <- readRDS('cycle_time_full_intx_nlq.rds')
ct_model_obs_info <- list(nobs = nobs(ct_model), 
                          norgs = ngrps(ct_model)$org_id_fac,
                          nusers = ngrps(ct_model)$`org_id_fac:user_id_fac`)
```

# Introduction

Understanding the factors that affect the productivity of software developers offers organizations the knowledge to deliver value to users, maintain competitiveness, and improve developer experience. Indeed, whether software is the an organizations primary product, or whether internal software engineering enables other end goals (such as in the pharmaceutical industry), the productivity of developers is increasingly the foundation on which all other activities are built. 

In software research, the time it takes to move through a work task is typically characterized as a measure of _velocity_. In calls to re-examine the complexity of developer productivity, researchers have argued that velocity measures are highly task-dependent, and do not represent the quality of work done or other, longer-term measures of the impact of work [@sadowskiSoftwareDevelopmentProductivity2019]. Nevertheless, time measures are very frequently used as an outcome measure to make recommendations for software engineering practices, e.g. in evaluating the perceived impact of technical debt [@beskerTechnicalDebtCripples2018]. These measures are also very inexpensive and convenient to collect for teams trying to use metrics to track productivity.

This report seeks to describe both the impact of several influential factors, as well as describe what cycle time looks like across a wide range of conditions. That is, understanding the variation in cycle time is as important as what influences it on average. We report on cycle time data for a full year, across hundreds of organizations, which allows us to examine fluctuations within a person's workflow as well as different stable tendencies between people.

RQ1. How do common software development process and workplace factors impact productivity? 

RQ2. How much between- and within-person variation is there in productivity?

# Background

## Defining productivity

There is disagreement about what constitutes productivity. Perceptions of what counts as productivity differ across individuals and roles, with managers tending to focus on long-term outcomes and ICs focusing on activity, for example [@storeyHowDevelopersManagers2022]. The scientific literature on this topic has used code-based metrics such as cycle time, commits, or lines of code[@murphy-hillWhatPredictsSoftware2021a]. These have obvious limitations in that the meaning of a particular unit for any of these metrics may be different depending on context [@sadowskiSoftwareDevelopmentProductivity2019]. Some researches have sought solutions to this problem by asking individuals to rate their own level of, or satisfaction with, productivity [@storeyTheorySoftwareDeveloper2021]. While it is plausible that perceived productivity could indicate productivity _per se_, such a measure is not free from context effects that would impact more "objective" metrics, and presents its own set of measurement challenges. 

Another difficulty stems from an often confused distinction between production (quantity of output regardless of resources provided), productivity (quantity of output given the resources provided), and performance (flexibility, adaptability, dependability, sustainability, and quality of output over time) [@leeOurOrganizationActually2023]. While the present analysis necessarily identifies production as the outcome, the influence of differernt factors on production highlights the need for these distinctions. For example, we may see that production velocity differs depending on the proportion of work that consists of defect tickets, which highlights the need to consider this contextual factor when considering individual performance.

# Research design and methodology

All code can be found in the repository for this paper.

## Data Selection and Characteristics

To examine coding time, task scoping, and collaboration as predictors of cycle time over time, we pulled data from the software metrics tool, Flow. Flow is a software platform that leverages git and ticketing data to track individual, team, and organization-level metrics that can be used as indicators of production, productivity, and performance over time. From Flow, we selected data for analysis if users actively contributed code. The data thus consisted of `r scales::comma(ct_model_obs_info$nobs)` observations across 12 months from `r scales::comma(ct_model_obs_info$nusers)` in `r scales::comma(ct_model_obs_info$norgs)` organizations of varying sizes and industries.

## Defining Study Variables

Using the most complete data for each user, we use the mean to aggregate each variable at the month level and the year level (see below for more details specific to each variable). For each predictor, we then subtracted each person's yearly average from their monthly data to produce a within-person deviation variable. This allows us to disaggregate effects on the outcome due to yearly-level individual differences and within-person, month-to-month fluctuations [@curranDisaggregationPersonPerson2011]. This important step allows us to avoid averaging between-person and within-person differences into a single effect estimate. These effects can be different even in the sign. Take the trivial example of typing speed and errors: for any given person, the faster they type, the more likely they are to make an error; however, typists who are faster on average tend to be more experienced and make fewer errors.

### Cycle Time

This is the dependent variable in these analyses. After computing the cycle time for each closed ticket in seconds, we found the median cycle time for each month for each user using all tickets _opened_ in that month. For example, a ticket opened on the 9th of April, and closed on the 3rd of May would contribute 2,246,400 seconds to the calculation of the median for April.

### Unclosed Tickets

We were not able to observe the closing date for every ticket given our data collection cutoff of ***DATE***, and so it is plausible that we underestimate the median cycle time in a way that depends in part on how many ticket closing times we do not observe. For this reason, we also computed the proportion of tickets opened in that month that had not been closed by the end of our data collection. For example, any ticket opened in April but not closed by ***DATE*** would count toward the proportion of unclosed tickets for that month.

### Coding Time

The amount of coding time was summarized as the average number of days per week that a developer made at least one commit. We divided the number of coding days in a month by the total number of days in that month and multiplied by seven to aid in interpretation.

### Total Merged PRs

Task scoping is a potentially important driver of cycle time. To measure the extent to which smaller chunks of work lead to a completed ticket, we counted the number of merged pull requests for each user for each month.

### Percent Defect Tickets

Defect tickets represent unplanned work that may interfere with timely completion of planned work. To account for this possibility, for each user, for each month, we computed the proportion of tickets that were defect tickets.

### Degree centrality

We measured collaboration by calculating degree centrality. To evaluate degree centrality, a metric derived from network analysis, we employed a framework where developers were treated as nodes within the network, and their interactions in the form of Pull Requests (PRs) were regarded as connections. Within this analysis, we measured the degree centrality of each developer by assessing the number of other developers with whom they had collaborated throughout the entirety of 2022. To ensure a normalized assessment, this value was then divided by the total number of developers constituting the organizational network. The calculations were executed using the Python package Networkx. This particular variable serves as an effective proxy for quantifying the extent of collaboration amongst developers.

### Comments per PR

Another indicator of collaboration is the frequency of comments within PRs. We undertook a comprehensive examination of all PRs that were successfully merged in the year 2022, meticulously calculating the average word count contained within each PR. This served as a valuable measure to gauge the depth of collaboration exhibited during the development and review process.

| Broad Construct | Variable                          | Variable Description                                                 |
|:-----------------------|:------------------|:------------------------------|
| Coding Time     | Coding Days per Week              | Avg number of coding days a week                                     |
| Task Scoping    | Total Merged PRs                  | Total number of merged PRs per developer                             |
|                 | Percent Defect Tickets            | Percent of all tickets that are defect tickets                       |
| Collaboration   | Degree Centrality                 | Score based on the number of reviewers a developer has worked with   |
|                 | Comments per PR                   | Number of comments per pr a developer is the author on               |
| Productivity    | Cycle Time                        | Avg time from ticket start to end                                    |
|                 | Proportion unclosed tickets       | Control variable to account for tickets missing cycle time           |

: Construct and variable descriptions {#tbl-constructs}

## Analytic Approach



We fit the above models using `brms` [v2.21.6, @burknerAdvancedBayesianMultilevel2018; @burknerBrmsPackageBayesian2017], interface the the Stan probabilistic programming language for Bayesian sampling [v2.35.0, @gelman], with the `cmdstanr` backend [v0.8.0, @gabryCmdstanrInterfaceCmdStan2024], in R [v4.3.2, @rcoreteamLanguageEnvironmentStatistical2023].

# Results


```{r model summary table, eval=TRUE}
ct_model_draws <- brms_to_draws(ct_model, variables = variables(ct_model)[1:37])
ct_model_pars <- parameters::model_parameters(ct_model_draws, centrality = 'median', ci = .95, ci_method = 'hdi')
model_param_table <- kableExtra::kbl(ct_model_pars, digits = 2, format = 'pipe', booktabs = TRUE, tabular = TRUE, caption = "Model parameter estimates")

if(knitr::is_html_output()){
  knitr::knit_print(kableExtra::kable_styling(model_param_table, bootstrap_options = c('striped', 'hover')))
} else {
  knitr::knit_print(model_param_table)
}
```

# Limitations

Factors outside our control that organizations can control to increase the reliability and validity of their cycle time data include establishing a systematic approach to ticket use. Even within an organization, individual teams may have different cultures regarding when it is appropriate to open and close a ticket. If one is using cycle time as a metric to guage team velocity, it may be ideal to open and close tickets based on consisten criteria across teams. For example, opening a ticket after a piece of work is completed, and then closing it soon after, does not provide a good measure of how long that work took to complete. Distinguishing ticket types may also help. Here we were able to separate tickets into defect and non-defect tickets, but there are certainly more granular ways of categorizing tickets. These categories, in an analysis like the above, can help ensure the analytic model is comparing like with like. 

# Discussion

## Many theories of productivity

::: {.callout-important}
I'm leaving this in for now because at the end of it I've kind of gotten to a good place in trying to situate the meaning of our findings in the production-versus-productivity space.
:::

In order to have a model of productivity, one must have a theory of productivity (and indeed, model building can be a great tool for theory building). As pointed out by contemporary philosophers of science [@alexandrova], such a theory need not rely on a single, all encompassing concept (or philosophy) of the phenomenon under investigation. However, a too-situated theory of productivity would fail to add to general knowledge and be less useful than it could be. Creating a theory of productivity in the midrange between the unitary and the too-specific does require a good deal of philosophical work to establish necessary and sufficient conditions for what counts as productivity for a particular context, and this work has not been done. It is only once this work has been done that we can begin to evaluate the validity of the tools we use to quantify and measure the phenomenon. 

Some readers may object that productivity can mean so many things that the best we can do is hyper-specific theories, and others might object that the work of defining productivity is so basic that it's not worth doing. Take for example a candidate theory that tried to map closely on industrial examples: productivity is just the number of component parts per time interval of a salable unit produced by a worker. It is sufficient to consider something a part that can be handed off to the next stage of production, and if the creation of this part is something that is repeatedly produced. We might say it's sufficient if all parts are similar rather than identical. This is nice because it's easy to measure, and has a direct relationship to what is being sold, and for how much it's being sold. We might add a necessary condition that the cog so produced actually works, i.e., that it's free from defects. We'll say it's sufficient that the cog is produced even if the eventual widget isn't ever sold because the worker has no control over this. For software developers we simply need to find the proper meaning of "part". For measuring productivity in software development, clearly, a closed ticket initiated by the developer fits the bill. It's a complete part that is passed to the next stage of production. We can imagine implementing a measure of quality of ticket completion (if the ticket has to do with delivered code, a reviewed pull request, for example). What of the condition that the parts are all similar? We have to have a concept of similarity, and this is where a good deal of trouble has arisen for software research.

Again, tackling this problem may seem overly burdensome or trivial, depending on the reader. But as the literature shows, it's an important problem. We can, perhaps, start by considering very easy ways of breaking up tickets, as we've done in this manuscript, by separating defect tickets from all other tickets. Even here, though, as discussed, defect tickets can be extremely different. We are not going to solve the problem of similarity here. It is worth pointing out that this problem might be solved more or less easy depending on the needs of the context, however. Considering productivity as a thing that is only comparable within person during a period when they are generally doing the same kind of work in the same environment, we can relax the idea of similarity and let differences in ticket complexity wash out in the average over long enough time spans. As a measurement concern, we would have to ensure that the way tickets are written does not vary (in the sense that one ticket now is broken up into 10 tickets for each subtask down the line), but this is a fairly technical consideration in defining the measurement. In this case, similarity would be satisfied simply if the ticket is assigned to the same person in the same context in the same role as other tickets. However, if we consider the context where we need the theory to allow us to make comparisons across many people, we might find that the similarity condition is insolvable. We should decide whether or not this is the case, and whether it makes sense to revise our theory in light of this difficulty. Perhaps this tricky problem of comparing like with like sinks the whole thing. That is crucial information to have before a single study has even been run.

Interestingly, this is one of the implicit concessions that has made in recent research that uses self-reported productivity. In place of an explicit theory about what productivity is, researchers in these cases rely on whatever our private folk theory is of productivity. Often the question is just a single item such as "how satisfied are you with your level of productivity." The conceptual strengths and weaknesses of these folk theories are left in the privacy of each study participant's mind. 

Another challenge in software development is the level of autonomy of each worker. Usually, these workers are not being handed tickets with one obvious pattern for completion. Many tickets require quite a bit of creativity. And the generation of tickets themselves is the result of better or worse planning, with the planning also being driven by the worker completing the tickets. Completing a ticket in a certain way may provide robustness against future software changes which is not strictly an issue of quality control. Given the level of autonomy of some software developers it really might make sense to no longer ignore the market performance of the product to which that developer contributed. I am not suggesting that producivity _should_ be defined with or without this consideration, but rather highlighting that for a particular theory, this choice must be considered. Software developers also have to ensure that their part of the work plays nicely with all the other parts of the work, which is not the case when one is making the same cog. "Playing nicely" means reducing the amount of extra effort that needs to be expended by teammates. This isn't strictly quality control in the sense that one might throw out a one bent cog and not count it toward total parts produced. The developer may also make choices that delight, or disgruntle, end users, leading to changes in customer ratings and sales. We can decide that what we mean by productivity either does or does not have to consider these aspects of the work produced. If we decide that our theory should consider these aspects, it has obvious implications for our measurement, for the kinds of things we look for as levers on productivity, and the kinds of recommendations we ultimately make to the software development community. 

Some readers might see existing literature as already doing what we propose, above. For example, the SPACE model of developer productivity purports to capture "the most important dimensions of developer productivity." From the introduction, it sounds like the authors are attempting to solve the problem of a lack of definitions of productivity, which is exactly the task we've been describing, above. The SPACE framework accurately conveys the complexity of this problem. However, first, it conflates predictors of productivity (satisfaction, efficiency, collaboration) with productivity per se. Second, it does not lay out clearly what should count as productivity. While the remaining dimensions seem relevant to a definition (activity, performance), these concepts are defined primarily through examples. Activity is defined via examples of the kind discussed above (e.g., "volume or count of design documents"), while performance is defined as potentially being either quality or impact (two distinct aspects discussed above). The authors, while hinting that their goal is a definition of productivity, do not claim to have a theory of productivity. And this paper is just an example of a lot of the best work in this area. It is clarifying to be able to walk through the concept space surrounding this phenomenon, and I am grateful to these authors for organizing and publishing their efforts. It is not a theory, and does not do as much as one would hope for directing a research program. Most importantly it does not provide a strong guide for what counts and what doesn't, nor does it provide a discussion of the implications of that definition.

In the present manuscript, we have again sidestepped the problem of providing a theory of productivity. We say, well, cycle-time is a fine way to examine velocity, which is related to production, which is related to productivity. We already see hints that what we mean by productivity might differ depending on the kinds of work one is doing over the course of a year (yearly average proportion of tickets that are defect tickets), or fluctuations day to day (the same variable aggregated within months). Do we think production or productivity differs depending on whether a ticket is a defect ticket or not? Clearly this matters for production, but we would be less sure about making a statement regarding productivity, in part because we can imagine a person being equally productive when working on a defect ticket versus a non-defect ticket (whatever we think productivity really is). On the other hand, we can also imagine that a person who isn't super excited about implementing features but loves squashing bugs really does change their level of productivity depending on the ticket type. 

# Related Work

# Conclusions

# References {.unnumbered}

