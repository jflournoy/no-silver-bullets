---
title: Understanding Technology Team Velocity at Scale
author:
  - name: John C. Flournoy
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: true
    email: jcflournoyphd@pm.me
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
  - name: Carol S. Lee
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: false
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
  - name: Catherine M. Hicks
#    orcid: 0000-0002-0760-5497
    #   - Investigation
    #   - Project administration
    #   - Software
    #   - Visualization
    corresponding: false
    email: cat-hicks@pluralsight.com
    # roles:
    affiliation:
      name: Pluralsight
      city: Draper, UT
      country: USA
# keywords:
abstract: |
  Things
plain-language-summary: |
  Other things
key-points:
  - A point
date: last-modified
bibliography: references.bib
number-sections: true
acm-metadata:
  # comment this out to make submission anonymous
  
  # if uncommented, this produces a teaser figure
  # 
  # anonymous: true
  # comment this out to build a draft version
  final: false
  # comment this out to specify detailed document options
  # acmart-options: sigconf, review  
  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: none
  # doi: XXXXXXX.XXXXXXX
  # conference-acronym: "Conference acronym 'XX"
  # conference-name: |
  #   Make sure to enter the correct
  #   conference title from your rights confirmation email
  # conference-date: June 03--05, 2018
  # conference-location: Woodstock, NY
  # isbn: 978-1-4503-XXXX-X/18/06
  # if present, replaces the list of authors in the page header.
  shortauthors: Flournoy et al.
  acks: Maggie Wu
  # Please copy and paste the code instead of the example below.
  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  ccs: |
        \begin{CCSXML}
        <ccs2012>
           <concept>
               <concept_id>10002944.10011123.10010912</concept_id>
               <concept_desc>General and reference~Empirical studies</concept_desc>
               <concept_significance>500</concept_significance>
               </concept>
         </ccs2012>
        \end{CCSXML}
        
        \ccsdesc[500]{General and reference~Empirical studies}
  keywords:
    - productivity
    - cycle time
    - collaboration
    - individual differences
    - within-person variation
  # teaser:
  #   image: sampleteaser
  #   caption: figure caption
  #   description: teaser description   
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE)
library(knitr)
library(flextable)
library(posterior)
library(parameters)
library(scales)
ct_model_obs_info <- readRDS('cycle_time_full_intx_lin_remonth_obs-info.rds')
```


# Introduction

Understanding the factors that affect the productivity of software developers offers organizations the knowledge to deliver value to users, maintain competitiveness, and improve developer experience. However, productivity is a difficult phenomenon to define and measure well. As a proxy for productivity, more objective measures of output have the advantage of having a concrete referent that is often simple to measure as a way for organizations and individuals to take control of tracking their output. In this report, we examine the relation between _cycle time_ (the duration between a ticket's opening and closing) and a number of workplace and process factors that have to do with task scoping, focused work time, collaboration, and time of year.

Cycle time is a measure of _velocity_, which in software research is defined as the time it takes to move through a work task. In calls to re-examine the complexity of developer productivity, researchers have argued that velocity measures are highly task-dependent, and do not represent the quality of work done or other, longer-term measures of the impact of work [@sadowskiSoftwareDevelopmentProductivity2019]. Nevertheless, time measures are very frequently used as an outcome measure to make recommendations for software engineering practices, e.g. in evaluating the perceived impact of technical debt [@beskerTechnicalDebtCripples2018]. These measures are also very inexpensive and convenient to collect for teams trying to use metrics to track productivity.

Although productivity _per se_ is not expected to cause cycle time in the same way across all contexts, there is a necessary relationship between the two. Lower cycle times indicate faster delivery times and more efficient software processes, cycle time has also become a key indicator of team health, developer productivity, and team efficiency [@clincySoftwareDevelopmentProductivity2003]. While we focus specifically on cycle time, what we learn may have implications for productivity. We analyze cycle time data for a full year across hundreds of organizations, allowing us to examine fluctuations within a person's workflow as well as different stable tendencies between people. Our research questions are:

RQ1. How do common software development process and workplace factors impact cycle time? 

RQ2. How much between- and within-person variation is there in cycle time?

# Background

## Productivity

There is disagreement about what constitutes productivity. Perceptions of what counts as productivity differ across individuals and roles, with managers tending to focus on long-term outcomes and ICs focusing on activity, for example [@storeyHowDevelopersManagers2022]. The scientific literature on this topic has used code-based metrics such as cycle time, commits, or lines of code[@murphy-hillWhatPredictsSoftware2021a]. These have obvious limitations in that the meaning of a particular unit for any of these metrics may be different depending on context [@sadowskiSoftwareDevelopmentProductivity2019]. Some researches have sought solutions to this problem by asking individuals to rate their own level of, or satisfaction with, productivity [@storeyTheorySoftwareDeveloper2021]. While it is plausible that perceived productivity could indicate productivity, such a measure is not free from context effects that would impact more "objective" metrics, and presents its own set of measurement challenges. 

Another difficulty stems from an often confused distinction between production (quantity of output regardless of resources provided), productivity (quantity of output given the resources provided), and performance (flexibility, adaptability, dependability, sustainability, and quality of output over time) [@leeOurOrganizationActually2023]. The present analysis identifies cycle time, a measure of production, as the outcome, and in doing so is able to highlight distinctions that likely no only matter for production but also point to areas of distinction that we should consider when thinking about individual productivity.

## Ticket assignment

The kind of ticket is likely an important factor in how tickets are completed. Assignment of tickets, especially defect tickets, is complex over time and both the initial and subsequent assignments of tickets can be impacted by many factors. For example, after an initial assignment to a developer, bugs may frequently be reassigned due to diverse factors such as determining ownership, time constraints, and identifying developers who may have a particular view into the root cause of the bug [@guoNotMyBug2011]. Some individuals may get assigned defect tickets more often than others, and changes in the proportion of work that consists of defect tickets may change how fast someone is able to work in general, either because defect tickets tend to be larger or smaller in scope than other tickets, or because of disruptions.

## Evaluating individual developer performance 

Defining developers’ productivity and performance is a highly contentious exercise and many different definitions are given by both practitioners and researchers [@murphy-hillWhatPredictsSoftware2021a; @sadowskiSoftwareDevelopmentProductivity2019].

Across workplaces, measures of time are frequently used to assess productivity even while the shortcomings of these measures are also widely acknowledged, and alternative measures include self-ratings or peer evaluations [@ramirezMeasuringKnowledgeWorker2004] and in software engineering, operationalizations of code work such as lines of code [@murphy-hillWhatPredictsSoftware2021a].

We lack holistic evidence about what practitioners in software development believe about developer performance and ability; some accounts with samples at large technology companies have suggested both that definitions of productivity can vary widely between managers and developers, and that software developers perceive many potential trade-offs between types of technical goals, e.g. that quality and speed may be unattainable together [@storeyHowDevelopersManagers2022].

One “industry myth” which is referenced frequently in practitioner commentary is the idea of a “10x engineer”: this position alleges that some small outlier population of software developers consistently outperform others on key development tasks. Potentially springing from small case studies examining a handful of developers’ time spent solving small laboratory tasks [@sackmanExploratoryExperimentalStudies1968; discussed in @nicholsEndMythIndividual2019], this “law” was generalized from only twelve individuals, uses time spent on the tasks as an estimate of both effort and cost, has failed to replicate in larger examinations of developer performance on similar tasks, and failed to acknowledge large within-individual variation in task performance [@nicholsEndMythIndividual2019; @shrikanthAssessingPractitionerBeliefs2021].

Nevertheless, the idea that "10x engineers" exist and that some individuals in software engineering outperform others by a "rule" of 10x has been cited often and codified in industry commentary, [e.g., @brooksMythicalManmonthEssays1975]. Modern commentary on this idea frequently refers to it as a myth, but it is also discussed as a potentially real phenomenon^[For example, see this [ycombinator thread](https://news.ycombinator.com/item?id=22349531) ([internet archive](http://web.archive.org/web/20240917164935/https://stackoverflow.blog/2024/06/19/the-real-10x-developer-makes-their-whole-team-better/)), and this [StackOverflow blog post](https://stackoverflow.blog/2024/06/19/the-real-10x-developer-makes-their-whole-team-better)  ([internet archive](http://web.archive.org/web/20231209171051/https://news.ycombinator.com/item?id=22349531))]. In our previous work, we have noted that some software practitioners hold field-specific ability beliefs that software development success and productivity is attributable to a quality of “innate brilliance”, and that this belief among practitioners may create a higher likelihood of experiencing threat and anxiety in the face of rapid role change and technological shifts to developer workflows [@hicksNewDeveloperAI2024]. 

# Research design and methodology

All code can be found in the repository for this paper.

## Data Selection and Characteristics

To examine coding time, task scoping, and collaboration as predictors of cycle time over time, we centered our analysis on a large, real-world dataset of git and ticketing data of `r scales::comma(ct_model_obs_info$nobs)` observations across 12 months from `r scales::comma(ct_model_obs_info$nusers)` users in `r scales::comma(ct_model_obs_info$norgs)` organizations of varying sizes and industries. This data was available via partnerships between a software metrics tool which was incorporated into the workflows of real working software teams, and the 216 organizations which opted in to this tool at any point during the 12 month analytic window. Notably, users themselves did not have to be active users of the software metrics tool itself in order to be included in this dataset, and git and ticketing data was available retrospectively for dates prior to the implementation of the tool in the organization. In other words, the git and ticketing data included in this analysis is not predicated on being an individual user of the software metrics tool, nor on the software metrics tool being used at the organization, as our dataset contains measures both before and after the software metric tool implementation at the organization, and implementation dates for organizations vary across the 12 month period.

Data were selected for analysis based on whether users actively contributed code during the time frame of the study. The 216 organizations included in this dataset had between [N - N] users, and in previous pilot surveys used to inform the design of this project, professional software developer users from these organizations described their main industries as ranging from Technology, Finance, Government, Insurance, Retail, and others, indicating a wide diversity of business use cases and engineering contexts were present in this sample. 

## Computing study variables

Using the most complete data for each user, we use the mean to aggregate each variable at the month level and the year level (see below for more details specific to each variable). For each predictor, we then subtracted each person's yearly average from their monthly data to produce a within-person deviation variable. This allows us to disaggregate effects on the outcome due to yearly-level individual differences and within-person, month-to-month fluctuations [@curranDisaggregationPersonPerson2011]. This important step allows us to avoid averaging between-person and within-person differences into a single effect estimate. These effects can be different even in the sign. Take the trivial example of typing speed and errors: for any given person, the faster they type, the more likely they are to make an error; however, typists who are faster on average tend to be more experienced and make fewer errors. All year-level individual differences variables were centered at their mean. Exceptions or addenda are mentioned below. See @tbl-variables for a brief list of variables.

### Cycle Time

This is the dependent variable in these analyses. After computing the cycle time for each closed ticket in seconds, we found the median cycle time for each month for each user using all tickets _opened_ in that month. For example, a ticket opened on the 9th of April, and closed on the 3rd of May would contribute 2,246,400 seconds to the calculation of the median for April.

### Unclosed Tickets

We were not able to observe the closing date for every ticket given our data collection cutoff of March 7, 2023, and so it is plausible that we underestimate the median cycle time in a way that depends in part on how many ticket closing times we do not observe. For this reason, we also computed the proportion of tickets opened in that month that had not been closed by the end of our data collection. For example, any ticket opened in April but not closed by March 7, 2023 would count toward the proportion of unclosed tickets for that month. We transformed proportions from $[0,1]$ to $(-\infty, \infty)$ using the logistic quantile function (with minum and maximum proportions forced to be .01 and .99 respectively).

### Time (Month, and within-quarter month)

We include month number as our measure of time. Additionally, we include the within-quarter month number to account for any effects of quarterly cycles. Month number was centered at month 7, and within-quarter month was centered at month 2.

### Team Size

To control for any influence of team size on cycle time, we compute each individual's team size as the average size of all teams that individual belongs to.

### Coding days

The amount of coding time was summarized as the average number of days per week that a developer made at least one commit. We divided the number of coding days in a month by the total number of days in that month and multiplied by seven to aid in interpretation.

### Total Merged PRs

Task scoping is a potentially important driver of cycle time. To measure the extent to which smaller chunks of work lead to a completed ticket, we counted the number of merged pull requests for each user for each month.

### Percent Defect Tickets

Defect tickets represent unplanned work that may interfere with timely completion of planned work. To account for this possibility, for each user, for each month, we computed the percent of tickets that were defect tickets.

### Degree centrality

We measured collaboration by calculating degree centrality. To evaluate degree centrality, a metric derived from network analysis, we employed a framework where developers were treated as nodes within the network, and their interactions in the form of Pull Requests (PRs) were regarded as connections. In other words, any contribution of code to the same pull request constituted a collaboration edge between developers. We normalized each centrality value by dividing by the total number of developers constituting the organizational network. The calculations were executed using the Python package Networkx (CITATION). This particular variable serves as an effective proxy for quantifying the extent of collaboration among developers. We multiply the normalized degree centrality, which is between 0 and 1, by 100.

### Comments per PR

::: {.callout-warning}
Is this still an accurate description of this var? This is from the research brief but I have been thinking of this as a number of comments. However, average word count would make some sense as well given the large values some observations have. But the sql does not seem to be word count.
:::

Another indicator of collaboration is the frequency of comments within PRs. We undertook a comprehensive examination of all PRs that were successfully merged in the year 2022, meticulously calculating the average word count contained within each PR. This served as a valuable measure to gauge the depth of collaboration exhibited during the development and review process. This value was an indication of the collaboration on PRs on which an individual was the author of the PR.

|                 | Variable                          | Variable Description                                                 |
|:----------------|:----------------------------------|:---------------------------------------------------------------------|
| Productivity    | Cycle Time                        | Avg time from ticket start to end                                    |
|                 | Proportion unclosed tickets       | Control variable to account for tickets missing cycle time           |
| Time            | Month                             | Continuous time variable coded as month number                       |
|                 | Within-quarter month              | Index of the month number within each quarter year                   |
| Team Context    | Team size                         | Average size across all teams a individual is on                     |
| Coding Time     | Coding Days per Week              | Avg number of coding days a week                                     |
| Task Scoping    | Total Merged PRs                  | Total number of merged PRs per developer                             |
|                 | Percent Defect Tickets            | Percent of all tickets that are defect tickets                       |
| Collaboration   | Degree Centrality                 | Score based on the number of reviewers a developer has worked with   |
|                 | Comments per PR                   | Number of comments per pr a developer is the author on               |

: Variable descriptions {#tbl-variables}

## Analytic Approach

The models described below are fit using `brms` [v2.21.6, @burknerAdvancedBayesianMultilevel2018; @burknerBrmsPackageBayesian2017], interface the the Stan probabilistic programming language for Bayesian sampling [v2.35.0, @gelman], with the `cmdstanr` backend [v0.8.0, @gabryCmdstanrInterfaceCmdStan2024], in R [v4.3.2, @rcoreteamLanguageEnvironmentStatistical2023].

We develop a model of monthly average ticket cycle time conditional on the above predictors. Specifically, we model cycle time as distributed Weibull with two parameters, $\lambda$ (scale), and _k_ (shape). The Weibull distribution is often used to model time-to-event data [@harrellRegressionModelingStrategies2015; @rummelAverageWeibullAnalysis2017], where _k_ determines the change over time in the probability of an event occurring (often called the "hazard rate"), and where $\lambda$ determines the time-to-event for some proportion of the cases (or in other words, how spread out the distribution is). In our case (and as is common practice), we are interested in determining the influence of our covariates on the scale of the distribution of cycle times, but we also allow the shape, _k_, to vary across organizations to account for different general tendencies in the change of the probability of ticket closer. In short, the Weibull distribution provides flexibility for accurately describe cycle time data that tend to have a bulk of observations at the low end, with a very long tail of more extreme observations.

The model for $\lambda$ is

\begin{equation}
\begin{aligned}
\log(\lambda) &= X\beta + \eta_{\text{org}} + \eta_{\text{org:user}} \\
\eta_{\text{org}} &\sim \mathcal{N}\left(\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \begin{bmatrix} \sigma_{11} & \sigma_{12} \\ \sigma_{21} & \sigma_{22} \end{bmatrix}\right) \\
\eta_{\text{org:user}} &\sim \mathcal{N}\left(\begin{bmatrix} \mu_3 \\ \mu_4 \end{bmatrix}, \begin{bmatrix} \sigma_{33} & \sigma_{34} \\ \sigma_{43} & \sigma_{44} \end{bmatrix}\right) 
\end{aligned}
\end{equation}


where $X$ is the matrix of predictors, $\beta$ is the vector of coefficients, $\eta_{\text{org}}$ is random intercepts with mean $\mu_1$ and linear slopes of month with mean $\mu_2$ for each organization, and $\eta_{\text{org:user}}$ is random intercepts with mean $\mu_3$ and linear slopes of month with mean $\mu_4$ for each user nested within organization. The specific predictors in $X$ are within-quarter month number, team size, proportion of unclosed tickets, month number, yearly means and month-level deviations for coding days per week, total merged PRs, defect ticket percentage, degree centrality, and comments per PR. We also include interactions between month number and the following: team size, proportion unclosed tickets, and each of the yearly mean predictors. This allows us to account as completely as possible for our control variables (team size and proportion unclosed tickets), and allow the effect of month on cycle time to vary by the individual differences variables (e.g., to account for the possibility that someone who has higher coding days per week shows a less steep decrease in cycle time across the year than someone with lower coding days per week).

The model for _k_ is 

\begin{equation}
\begin{aligned}
\log(k) &= \zeta_{\text{org}} \\
\zeta_{\text{org}} &\sim \mathcal{N}(\mu_5, \sigma_{5}) 
\end{aligned}
\end{equation}


where $\zeta_{\text{org}}$ is a random intercept with mean $\mu_5$ each organization.

Conceptually, this model allows a unique distribution of cycle times (as determined by the random intercepts for both $\lambda$ and _k_) for each organization. It also allows the scale of the distribution of cycle times to vary for each user due to the random intercept for $\lambda$. The effect of time (month number) on the scale of the distribution of cycle times is also allowed to vary across organizations as well as users due to the random slopes (with means $\mu_2$ and $\mu_4$). This strategy allows two advantages: first, we account for multiple sources of variance that allows our estimates of the effects of interest to be more precise; and second, we are able to provide estimates of this variation across organizations and users. This variation itself is of interest given the various myths mentioned in the introduction about developer performance.

We model the effect of proportion of unclosed tickets and month number as smooth functions of the covariate using thin-plate splines for increased flexibility [@woodGeneralizedAdditiveModels2017]. Briefly, thin plate splines allow for flexible, non-linear relationships between predictors and the response variable. These splines are penalized to prevent overfitting, balancing model flexibility and complexity. The interactions between month number and our control variables are parameterized as additional smooth funcitons of month number multiplied by these variables. While our focal model parameterizes the interactions between year-level means and month number as linear coefficiencts on multiplicative combinations between the two variables, we also examined a model that uses additional smooth functions of month number multiplied by these variables to allow for additional complexity. We provide the model output for this sensitivity analysis in a supplement.

We set weakly-informative priors centered at zero for all parameters, except for the intercept for $\lambda$ and _k_ which were centered on their approximate values in the data (consistent with the default behavior of `brms`). We performed prior-predictive checks to ensure our prior specification generated data that covered and exceded the space of our observations. Given the complexity of the model, we also specified initialization of parameters at small plausible values (e.g., zero for coefficients, .1 for standard deviations of random effects). Full prior and initialization specifications are available in the analysis code.

We sampled from 4 chains with 2,000 total iterations each, discarding the first 1,000 iterations as warmup. Inferences were made on 4,000 post-warmup draws from the posterior probality distribution from the 4 chains. Unless otherwise stated we describe the posterior of paramters and predictions using the median of the distribution, and characterize its variation using the highest posterior density interval (HDI) which is defined as the interval that contains a specified percentage (usually 95%) of the most probable values of the parameter [@kruschkeRejectingAcceptingParameter]. We make general descriptive inferences based on the probability that a parameter has the sign of the posterior density's median value. For example, if 80% of the posterior density of the slope of the effect of month on cycle time is of the same sign as the density's median, and that median is negative, we would say something like, "given the model and the data, there is an 80% chance that there is a decrease in cycle times across the year." 

## Other R packages used

- [ ] make sure to cite other packages here.

# Results

Results from the linear model reported below were highly similar to those in the more flexible non-linear model sensitivity analysis described above. Also note that parameters in the table are from a linear model for the distribution of log($\lambda$) and log(*k*), while model expectations are on the response scale and can therefore display curvature even while the model is linear.

```{r prep model summary table fe}
fe_par_nicenames <- c(b_Intercept = 'Intercept log(λ)',
                      b_shape_Intercept = 'Intercept log(k)',
                      b_within_quarter_month_num_c = 'Within-quarter month',
                      b_team_size_c = 'Team size',
                      b_wi_avg_coding_days_per_week = 'Avg. coding days/week (within-person)',
                      b_yr_avg_avg_coding_days_per_week_c = 'Avg. coding days/week',
                      b_wi_total_merged_prs = 'Total merged PRs (within-person)',
                      b_yr_avg_total_merged_prs_c = 'Total merged PRs',
                      b_wi_defect_tickets_pct_indiv = 'Defect tickets % (within-person)',
                      b_yr_avg_defect_tickets_pct_indiv_c = 'Defect tickets %',
                      b_wi_degree_centrality_monthly_100 = 'Degree centrality (within-person)',
                      b_yr_avg_degree_centrality_monthly_100_c = 'Degree centrality',
                      b_wi_comments_per_pr = 'Comments per PR (within-person)',
                      b_yr_avg_comments_per_pr_c = 'Comments per PR',
                      `b_yr_avg_avg_coding_days_per_week_c:month_num_c` = 'Avg. coding days/week × Month',
                      `b_yr_avg_total_merged_prs_c:month_num_c` = 'Total merged PRs × Month',
                      `b_yr_avg_defect_tickets_pct_indiv_c:month_num_c` = 'Defect tickets % × Month',
                      `b_yr_avg_degree_centrality_monthly_100_c:month_num_c` = 'Degree centrality × Month',
                      `b_yr_avg_comments_per_pr_c:month_num_c` = 'Comments per PR × Month')

ct_model_draws_fe <- readRDS('cycle_time_full_intx_lin_remonth_draws_id-fe.rds')
ct_model_draws_bs <- readRDS('cycle_time_full_intx_lin_remonth_draws_id-bs.rds')
ct_model_draws_re <- readRDS('cycle_time_full_intx_lin_remonth_draws_id-re.rds')
ct_model_draws_sds <- readRDS('cycle_time_full_intx_lin_remonth_draws_id-sds.rds')
ct_model_pars_fe <- parameters::model_parameters(ct_model_draws_fe, 
                                                 digits = 4,
                                                 centrality = 'median', 
                                                 ci = .95, 
                                                 ci_method = 'hdi', 
                                                 test = 'pd', exponentiate = FALSE)
ct_model_pars_bs <- parameters::model_parameters(ct_model_draws_bs, 
                                                 digits = 4,
                                                 centrality = 'median', 
                                                 ci = .95, 
                                                 ci_method = 'hdi', 
                                                 test = 'pd', exponentiate = FALSE)
ct_model_pars_re <- parameters::model_parameters(ct_model_draws_re, 
                                                 digits = 4,
                                                 centrality = 'median', 
                                                 ci = .95, 
                                                 ci_method = 'hdi', 
                                                 test = 'pd', exponentiate = FALSE)
ct_model_pars_sds <- parameters::model_parameters(ct_model_draws_sds, 
                                                 digits = 4,
                                                 centrality = 'median', 
                                                 ci = .95, 
                                                 ci_method = 'hdi', 
                                                 test = 'pd', exponentiate = FALSE)
ct_model_pars_fe_for_text <- ct_model_pars_fe
ct_model_pars_fe$Parameter <- fe_par_nicenames[ct_model_pars_fe$Parameter]
ct_model_pars_fe$pd <- sprintf('% 3.0f%%', ct_model_pars_fe$pd*100)

names(ct_model_pars_fe) <- c('Parameter', 'Posterior Median', 'Lower 95% HDI', 'Upper 95% HDI', 'Sign Probability')
```

We find that all measured factors, both individual-difference and within-person deviations, have a non-zero association with cycle time, with 100% of the posterior distribution for these parameters sharing the same sign (see @tbl-results-fe for point estimates and uncertainty intervals). Within-quarter month showed a very small (relative to other effects shown below) negative association with cycle time, indicating that time to ticket completion is shorter at the end of quarters (@fig-quarter). Team size showed had close to no effect on cycle time (@tbl-results-fe). Cycle times tended to decrease slightly over the year (@fig-month). The proportion of unclosed tickets, an important control variable, on average had close to no effect on our measure of average monthly cycle time but interacted with month (see supplement).

![Effect of within-quarter month. Background hexagons represent density of data, with darker colors indicating greater density. Lines are median posterior expectations, with 95% credible interval ribbons.](plots/Plot Full growth model with linear predictors interacting-2.png){#fig-quarter}

![Effect of month. Background hexagons represent density of data, with darker colors indicating greater density. Lines are median posterior expectations, with 95% credible interval ribbons.](plots/Plot Full growth model with linear predictors interacting-1.png){#fig-month}

```{r model summary table fe, eval=TRUE}
#| tbl-cap: Fixed effects {#tbl-results-fe}

flextable(ct_model_pars_fe) |>
    colformat_double(digits = 4) |>
    set_table_properties(layout = 'autofit') |>
    theme_booktabs(bold_header = TRUE) |>
    align(j = 5, align = 'right', part = 'all') |>
    add_footer_lines('Posterior Median: The median of the posterior probability density for the parameter is used as the parameter point estimate. 95% HDI: The 95% highest density interval is the region of the posterior distribution with the 95% most probable values for the parameter. Sign Probability: The proportion of the posterior distribution that has the same sign (direction) as the point estimate.')

```

Specifically, when individuals increased average coding days per week month-to-month they also tended to have lower cycle times, and individuals with more average coding days per week across the year tended to have lower cycle times (@fig-codingdays). The association between coding days and cycle time also tended to increase in strength across months, with `r sprintf('%.0f%%', 100*ct_model_pars_fe_for_text$pd[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_avg_coding_days_per_week_c:month_num_c'])` of the posterior in this direction.

![Effect of average coding days per week on cycle time. Background hexagons represent density of data, with darker colors indicating greater density. Lines are median posterior expectations, with 95% credible interval ribbons.](plots/Plot Full growth model with linear predictors interacting-5.png){#fig-codingdays}

More merged PRs was associated with lower cycle time for both individual average differences and within-person differences. This effect also may get stronger across the year with `r sprintf('%.0f%%', 100*ct_model_pars_fe_for_text$pd[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_total_merged_prs_c:month_num_c'])` of the posterior in this direction (@fig-mergedprs). 

![Effect of merged PRs on cycle time. Background hexagons represent density of data, with darker colors indicating greater density. Lines are median posterior expectations, with 95% credible interval ribbons.](plots/Plot Full growth model with linear predictors interacting-6.png){#fig-mergedprs}

The percent of defect tickets showed a negative association with cycle time for within-person deviations and a positive association for individual differences. In other words, individuals who tended to have more defect tickets as a proportion of their work across the course of the year also tended to have longer cycle times. However, for any given person, and increase in the proportion of defect tickets in a month was associated with lower cycle times (@fig-defecttickets). The interaction with month number for this effect was centered close to zero, with only `r sprintf('%.0f%%', 100*ct_model_pars_fe_for_text$pd[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_defect_tickets_pct_indiv_c:month_num_c'])` of the posterior in the negative direction with a fairly narrow distribution around zero (95% HDI = `r sprintf('[%.4f, %.4f]', ct_model_pars_fe_for_text[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_defect_tickets_pct_indiv_c:month_num_c', 'CI_low'], ct_model_pars_fe_for_text[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_defect_tickets_pct_indiv_c:month_num_c', 'CI_high'])`).

![Effect of percent defect tickets on cycle time. Background hexagons represent density of data, with darker colors indicating greater density. Lines are median posterior expectations, with 95% credible interval ribbons.](plots/Plot Full growth model with linear predictors interacting-7.png){#fig-defecttickets}

Degree centrality, both on average across the year individual differences and within-person deviations, showed a negative association with cycle time (@fig-degree). In other words, individuals who on average contribute code to PRs that have a lot of other contributors tend to have lower cycle times for tickets they own. Similarly, when individuals' collaboration on PRs increases in a given month, their cycle time tends to go down. This effect does not unambiguously strengthen or weaken across the year with `r sprintf('%.0f%%', 100*ct_model_pars_fe_for_text$pd[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_degree_centrality_monthly_100_c:month_num_c'])` of the posterior for the interaction effect having negative sign with a fairly narrow distribution around zero (95% HDI = `r sprintf('[%.4f, %.4f]', ct_model_pars_fe_for_text[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_degree_centrality_monthly_100_c:month_num_c', 'CI_low'], ct_model_pars_fe_for_text[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_degree_centrality_monthly_100_c:month_num_c', 'CI_high'])`).

![Effect of degree centrality on cycle time. Background hexagons represent density of data, with darker colors indicating greater density. Lines are median posterior expectations, with 95% credible interval ribbons.](plots/Plot Full growth model with linear predictors interacting-8.png){#fig-degree}

Finally, the number of comments per PR showed a positive association with cycle time. Individuals who tended to garner more comments on their PRs also tended to have higher cycle times, and within a given month, a higher number of comments per PR relative to a person's average was also associated with higher cycle times (@fig-comments). This effect also does not unambiguously strengthen or weaken across the year with `r sprintf('%.0f%%', 100*ct_model_pars_fe_for_text$pd[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_comments_per_pr_c:month_num_c'])` of the posterior for the interaction effect having negative sign with a fairly narrow distribution around zero (95% HDI = `r sprintf('[%.4f, %.4f]', ct_model_pars_fe_for_text[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_comments_per_pr_c:month_num_c', 'CI_low'], ct_model_pars_fe_for_text[ct_model_pars_fe_for_text$Parameter == 'b_yr_avg_comments_per_pr_c:month_num_c', 'CI_high'])`). 

![Effect of comments per PR on cycle time. Background hexagons represent density of data, with darker colors indicating greater density. Lines are median posterior expectations, with 95% credible interval ribbons.](plots/Plot Full growth model with linear predictors interacting-9.png){#fig-comments}

```{r model summary table re, eval=TRUE}
#| tbl-cap: Random effects {#tbl-results-re}

flextable(ct_model_pars_re) |>
    colformat_double(digits = 4) |>
    set_table_properties(layout = 'autofit') |>
    theme_booktabs(bold_header = TRUE) |>
    align(j = 5, align = 'right', part = 'all') |>
    add_footer_lines('Posterior Median: The median of the posterior probability density for the parameter is used as the parameter point estimate. 95% HDI: The 95% highest density interval is the region of the posterior distribution with the 95% most probable values for the parameter. Sign Probability: The proportion of the posterior distribution that has the same sign (direction) as the point estimate.')

```

```{r model summary table bs, eval=TRUE}
#| tbl-cap: Basis spline effects {#tbl-results-bs}

flextable(ct_model_pars_bs) |>
    colformat_double(digits = 4) |>
    set_table_properties(layout = 'autofit') |>
    theme_booktabs(bold_header = TRUE) |>
    align(j = 5, align = 'right', part = 'all') |>
    add_footer_lines('Posterior Median: The median of the posterior probability density for the parameter is used as the parameter point estimate. 95% HDI: The 95% highest density interval is the region of the posterior distribution with the 95% most probable values for the parameter. Sign Probability: The proportion of the posterior distribution that has the same sign (direction) as the point estimate.')

```

```{r model summary table sds, eval=TRUE}
#| tbl-cap: Spline wiggliness effects {#tbl-results-sds}

flextable(ct_model_pars_sds) |>
    colformat_double(digits = 4) |>
    set_table_properties(layout = 'autofit') |>
    theme_booktabs(bold_header = TRUE) |>
    align(j = 5, align = 'right', part = 'all') |>
    add_footer_lines('Posterior Median: The median of the posterior probability density for the parameter is used as the parameter point estimate. 95% HDI: The 95% highest density interval is the region of the posterior distribution with the 95% most probable values for the parameter. Sign Probability: The proportion of the posterior distribution that has the same sign (direction) as the point estimate.')

```

# Limitations

Factors outside our control that organizations can control to increase the reliability and validity of their cycle time data include establishing a systematic approach to ticket use. Even within an organization, individual teams may have different cultures regarding when it is appropriate to open and close a ticket. If one is using cycle time as a metric to gauge team velocity, it may be ideal to open and close tickets based on consistent criteria across teams. For example, opening a ticket after a piece of work is completed, and then closing it soon after, does not provide a good measure of how long that work took to complete. Distinguishing ticket types may also help. Here we were able to separate tickets into defect and non-defect tickets, but there are certainly more granular ways of categorizing tickets. These categories, in an analysis like the above, can help ensure the analytic model is comparing like with like. 

# Discussion

Better characterizing how teams move through their ticketing processes, and what factors impact ticket velocity, can suggest initial starting places for interventions to improve those times. However, it is just as important to note that certain software engineering outcomes may require decreasing the closure of tickets. These goal-directed decisions about setting metrics targets cannot be answered by the present analysis, but require greater organizational context, and the addition of more goal-aligned outcome measures. For example, imagine that the software developers within an organization rapidly triage and close defect tickets, showing a very low cycle time. However, the quality of those closures is poor, as measured by the longevity of the “fix,” and further, interrelated defects. For an organization to more holistically understand the state of “how we handle defects” and identify potential changes, it would be necessary to not simply monitor the velocity of defect tickets but to establish other metrics around the depth and coverage of a solution and the “lifecycle” of defects. Qualitative work (e.g., in-depth interviews or post-mortems with engineers who handle these scenarios) may also reveal unmeasured and unnoticed organizational features – for example, a cultural belief that working on defect tickets is less important than feature tickets, and associated pressure that developers experience from management to close them quickly.

This type of example underlines the classic measurement argument that no single measure should become an overarching target, devoid of context. In our previous work, we have echoed other organizational researchers in proposing that the success or failure of interventions seeking to make a measurable change in a given organization can be determined in part by the psychological and behavioral expectations, contexts and experiences of people (Brocker & Sherman, 2019; Hicks, 2024). In a mixed-methods observational study with software developers, we also noted that individuals can voice high agreement with the perceived utility of tracking software processes, while also reporting high rates of perceived “pitfalls” in the usage of those metrics (Hicks, Lee & Ramsey, 2023). While the present analysis concerns how to characterize cycle time and connections between other metrics operationalized from software development workflows, this investigation does not establish nor measure specific organizational goals and their attainment. A future line of research might incorporate measures of individual, team, and organizational goals with system measures such as cycle time and provide greater evidence about how changes in cycle time impact organizational goal achievement.

However, better understanding both what cycle time looks like within organizations, and how to explore cycle time, is still highly relevant to how we think about measuring software engineering work without using this measure as a gauge for goals. For example, our analysis demonstrates that there is considerable variation in the movement of tickets. Rather than establishing a single “ideal cycle time” to aim for, we establish that cycle times are an interesting outcome which associates with factors such as [x, y, z]. Coding days may be a particularly interesting intervention point for future studies to explore [........] 

In this study, we were limited in the amount of organizational context that we could observe from ticketing and git data. For example, at a certain time of year an organization might implement a new planning process, a new promotion plan, or other organizational changes. Matching an analytical approach such as the current one, with a specific organization’s knowledge of their own planning cycles, investments, and other contextual details, could provide valuable insights about the efficacy and impact of organizational change on software development processes. In order to perform such an analysis, we have found that it is important to take into account [x, y, z]. 


# Related Work

# Conclusions

# References {.unnumbered}

